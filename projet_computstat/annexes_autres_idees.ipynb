{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autres exp\n",
    "import os, numpy as np, torch, matplotlib.pyplot as plt\n",
    "\n",
    "def savefig(outdir, name, dpi=200):\n",
    "    path = os.path.join(outdir, name)\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"saved:\", path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_samples(\n",
    "    model, x_true, x_init, mask_obs,\n",
    "    *,\n",
    "    method=\"mwg\",                 # \"pseudo\" ou \"mwg\"\n",
    "    n_iters=20000,\n",
    "    burn_in=5000,\n",
    "    thinning=50,\n",
    "    warmup_pg=50,\n",
    "    proposal_scale=1.0,\n",
    "    max_saved=200,                # limite nb samples gardés\n",
    "):\n",
    "    \"\"\"\n",
    "    Retourne: samples_probs (S,B,1,28,28) sur CPU (float), et last_bin (B,1,28,28)\n",
    "    \"\"\"\n",
    "    x = x_init.clone()\n",
    "    z = None\n",
    "\n",
    "    if method == \"mwg\":\n",
    "        for _ in range(warmup_pg):\n",
    "            x_out, _ = pseudo_gibbs_step(model, x, mask_obs)\n",
    "            x = torch.bernoulli(x_out)\n",
    "        mu, logvar = model.encode(x.view(-1, 784))\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "\n",
    "    saved = []\n",
    "    last_bin = None\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        if method == \"pseudo\":\n",
    "            x_out, _ = pseudo_gibbs_step(model, x, mask_obs)\n",
    "        else:\n",
    "            x_out, z, acc = metropolis_within_gibbs_step(\n",
    "                model, x, z, mask_obs,\n",
    "                return_accept=True,\n",
    "                adaptive=False,\n",
    "                proposal_scale=proposal_scale\n",
    "            )\n",
    "\n",
    "        x = torch.bernoulli(x_out)\n",
    "        last_bin = x.detach().cpu()\n",
    "\n",
    "        keep = (t >= burn_in) and ((t - burn_in) % thinning == 0)\n",
    "        if keep:\n",
    "            saved.append(x_out.detach().cpu())\n",
    "            if len(saved) >= max_saved:\n",
    "                break\n",
    "\n",
    "    samples = torch.stack(saved, dim=0)  # (S,B,1,28,28)\n",
    "    return samples, last_bin\n",
    "\n",
    "def plot_uncertainty(samples_probs, mask_obs, x_true, x_init, idx=0, outdir=None, prefix=\"mwg\"):\n",
    "    \"\"\"\n",
    "    samples_probs: (S,B,1,28,28) CPU\n",
    "    \"\"\"\n",
    "    S = samples_probs.size(0)\n",
    "    mean = samples_probs.mean(dim=0)     # (B,1,28,28)\n",
    "    var  = samples_probs.var(dim=0)      # (B,1,28,28)\n",
    "\n",
    "    m = mask_obs.detach().cpu()\n",
    "    miss = (1 - m)\n",
    "\n",
    "    # uncertainty only on missing\n",
    "    var_miss = var * miss\n",
    "    ent = -(mean.clamp(1e-6, 1-1e-6)*torch.log(mean.clamp(1e-6,1-1e-6)) +\n",
    "            (1-mean).clamp(1e-6,1-1e-6)*torch.log((1-mean).clamp(1e-6,1-1e-6)))\n",
    "    ent_miss = ent * miss\n",
    "\n",
    "    # figures\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(14,3))\n",
    "    axes[0].imshow(x_true[idx].detach().cpu().squeeze(), cmap=\"gray\", vmin=0, vmax=1); axes[0].set_title(\"True\"); axes[0].axis(\"off\")\n",
    "    axes[1].imshow(x_init[idx].detach().cpu().squeeze(), cmap=\"gray\", vmin=0, vmax=1); axes[1].set_title(\"Masked init\"); axes[1].axis(\"off\")\n",
    "    axes[2].imshow(mean[idx].squeeze(), cmap=\"gray\", vmin=0, vmax=1); axes[2].set_title(\"Posterior mean\"); axes[2].axis(\"off\")\n",
    "    axes[3].imshow(var_miss[idx].squeeze(), cmap=\"viridis\"); axes[3].set_title(\"Var (missing)\"); axes[3].axis(\"off\")\n",
    "    axes[4].imshow(ent_miss[idx].squeeze(), cmap=\"viridis\"); axes[4].set_title(\"Entropy (missing)\"); axes[4].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    if outdir is not None:\n",
    "        savefig(outdir, f\"{prefix}_uncertainty_maps_idx{idx}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # scalar metrics to save\n",
    "    metrics = {\n",
    "        \"S\": S,\n",
    "        \"var_missing_mean\": float(var_miss.mean().item()),\n",
    "        \"entropy_missing_mean\": float(ent_miss.mean().item()),\n",
    "    }\n",
    "    return mean, var, metrics\n",
    "\n",
    "def plot_samples_grid(samples_probs, idx=0, n_show=16, outdir=None, prefix=\"mwg\"):\n",
    "    S = samples_probs.size(0)\n",
    "    take = min(n_show, S)\n",
    "    # pick evenly spaced samples\n",
    "    ids = np.linspace(0, S-1, take).astype(int)\n",
    "\n",
    "    cols = 8\n",
    "    rows = int(np.ceil(take / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(2*cols, 2*rows))\n",
    "    axes = np.array(axes).reshape(rows, cols)\n",
    "    for k in range(rows*cols):\n",
    "        ax = axes[k//cols, k%cols]\n",
    "        if k < take:\n",
    "            ax.imshow(samples_probs[ids[k], idx].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "            ax.set_title(f\"s{ids[k]}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Posterior samples (idx={idx})\")\n",
    "    plt.tight_layout()\n",
    "    if outdir is not None:\n",
    "        savefig(outdir, f\"{prefix}_samples_grid_idx{idx}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def exp4_uncertainty_and_samples(\n",
    "    model, x_true, device,\n",
    "    *,\n",
    "    mask_kind=\"top\",         # \"top\" or \"random50\"\n",
    "    missing_rate=0.5,        # used if random50\n",
    "    method=\"mwg\",\n",
    "    n_iters=20000, burn_in=5000, thinning=50,\n",
    "    warmup_pg=50,\n",
    "    proposal_scale=1.0,\n",
    "    idx=0,\n",
    "    outdir_root=\"results\"\n",
    "):\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    outdir = os.path.join(outdir_root, f\"exp4_uncertainty_{run_id}\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(\"OUT:\", outdir)\n",
    "\n",
    "    if mask_kind == \"random50\":\n",
    "        mask = make_random_mask(x_true, missing_rate=missing_rate)\n",
    "    else:\n",
    "        mask = make_structured_mask(x_true, kind=\"top\")\n",
    "    x_init = init_with_noise(x_true, mask)\n",
    "\n",
    "    samples_probs, last_bin = collect_samples(\n",
    "        model, x_true, x_init, mask,\n",
    "        method=method,\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        warmup_pg=warmup_pg,\n",
    "        proposal_scale=proposal_scale,\n",
    "        max_saved=300\n",
    "    )\n",
    "\n",
    "    mean, var, metrics = plot_uncertainty(samples_probs, mask, x_true, x_init, idx=idx, outdir=outdir, prefix=method)\n",
    "    plot_samples_grid(samples_probs, idx=idx, n_show=16, outdir=outdir, prefix=method)\n",
    "\n",
    "    # Save metrics + config\n",
    "    import pandas as pd, json\n",
    "    pd.DataFrame([{\n",
    "        \"mask_kind\": mask_kind,\n",
    "        \"missing_rate\": missing_rate,\n",
    "        \"method\": method,\n",
    "        **metrics\n",
    "    }]).to_csv(os.path.join(outdir, \"exp4_metrics.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(outdir, \"exp4_config.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"mask_kind\": mask_kind,\n",
    "            \"missing_rate\": missing_rate,\n",
    "            \"method\": method,\n",
    "            \"n_iters\": n_iters, \"burn_in\": burn_in, \"thinning\": thinning,\n",
    "            \"warmup_pg\": warmup_pg,\n",
    "            \"proposal_scale\": proposal_scale,\n",
    "            \"idx\": idx\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(\"DONE exp4:\", outdir)\n",
    "    return outdir\n",
    "\n",
    "# ---------- RUN exp4 (example) ----------\n",
    "_ = exp4_uncertainty_and_samples(\n",
    "    model=model,\n",
    "    x_true=x_true,\n",
    "    device=device,\n",
    "    mask_kind=\"top\",       # or \"random50\"\n",
    "    missing_rate=0.5,\n",
    "    method=\"mwg\",\n",
    "    n_iters=20000,\n",
    "    burn_in=5000,\n",
    "    thinning=50,\n",
    "    warmup_pg=50,\n",
    "    proposal_scale=1.0,\n",
    "    idx=0\n",
    ")\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def exp5_outdir():\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = os.path.join(\"results\", f\"exp5_mcmc_{run_id}\")\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    print(\"OUT:\", out)\n",
    "    return out\n",
    "\n",
    "def savefig(outdir, name, dpi=200):\n",
    "    path = os.path.join(outdir, name)\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"saved:\", path)\n",
    "\n",
    "def acf_1d(x, max_lag=200):\n",
    "    \"\"\"\n",
    "    Autocorrelation function (unbiased-ish) for 1D numpy array.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = len(x)\n",
    "    if n < 5:\n",
    "        return np.arange(max_lag+1), np.full(max_lag+1, np.nan)\n",
    "    x = x - x.mean()\n",
    "    var = np.dot(x, x) / n\n",
    "    if var <= 1e-12:\n",
    "        return np.arange(max_lag+1), np.zeros(max_lag+1)\n",
    "    ac = np.zeros(max_lag+1, dtype=np.float64)\n",
    "    ac[0] = 1.0\n",
    "    for k in range(1, max_lag+1):\n",
    "        ac[k] = np.dot(x[:-k], x[k:]) / (n * var)\n",
    "    return np.arange(max_lag+1), ac\n",
    "\n",
    "def ess_from_acf(x, max_lag=200):\n",
    "    \"\"\"\n",
    "    ESS approx via integrated autocorrelation time (Geyer truncation simple).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = len(x)\n",
    "    if n < 10:\n",
    "        return np.nan\n",
    "    lags, ac = acf_1d(x, max_lag=max_lag)\n",
    "    # truncate when ac becomes negative (simple)\n",
    "    pos = ac[1:]\n",
    "    m = 0\n",
    "    for v in pos:\n",
    "        if v <= 0:\n",
    "            break\n",
    "        m += 1\n",
    "    tau = 1 + 2*np.sum(pos[:m])  # integrated autocorr time\n",
    "    return n / tau if tau > 0 else np.nan\n",
    "\n",
    "def rhat(chains):\n",
    "    \"\"\"\n",
    "    Gelman-Rubin R-hat for list/array of shape (M, N).\n",
    "    chains: numpy array (M chains, N samples)\n",
    "    \"\"\"\n",
    "    chains = np.asarray(chains, dtype=np.float64)\n",
    "    # drop non-finite columns chainwise\n",
    "    # simplest: keep only finite samples per chain, truncate to min length\n",
    "    cleaned = []\n",
    "    for c in chains:\n",
    "        c = c[np.isfinite(c)]\n",
    "        cleaned.append(c)\n",
    "    m = len(cleaned)\n",
    "    n = min(len(c) for c in cleaned) if m > 0 else 0\n",
    "    if m < 2 or n < 10:\n",
    "        return np.nan\n",
    "    X = np.stack([c[:n] for c in cleaned], axis=0)  # (m,n)\n",
    "\n",
    "    chain_means = X.mean(axis=1)\n",
    "    chain_vars = X.var(axis=1, ddof=1)\n",
    "    W = chain_vars.mean()\n",
    "    B = n * chain_means.var(ddof=1)\n",
    "    var_hat = (n-1)/n * W + (1/n) * B\n",
    "    return np.sqrt(var_hat / W) if W > 0 else np.nan\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_trace(\n",
    "    model, x_true, x_init, mask_obs,\n",
    "    *,\n",
    "    method=\"mwg\",                 # \"pseudo\" | \"mwg\"\n",
    "    n_iters=12000,\n",
    "    burn_in=2000,\n",
    "    thinning=20,\n",
    "    warmup_pg=50,\n",
    "    proposal_scale=1.0,\n",
    "    idx_stat=0,                   # quel élément du batch pour stats K\n",
    "):\n",
    "    \"\"\"\n",
    "    Retourne dict avec:\n",
    "      steps_kept: list\n",
    "      logp_list : list (scalar logp MC est. mean over batch, mais à chaque sample gardé)\n",
    "      k_list    : list (#pixels_on sur missing, pour idx_stat)\n",
    "      acc_list  : list (MwG accept par itération gardée, sinon NaN)\n",
    "    \"\"\"\n",
    "    x = x_init.clone()\n",
    "    z = None\n",
    "    accs = []\n",
    "\n",
    "    if method == \"mwg\":\n",
    "        for _ in range(warmup_pg):\n",
    "            x_out, _ = pseudo_gibbs_step(model, x, mask_obs)\n",
    "            x = torch.bernoulli(x_out)\n",
    "        mu, logvar = model.encode(x.view(-1, 784))\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "\n",
    "    steps_kept, logp_list, k_list, acc_list = [], [], [], []\n",
    "    mask_cpu = mask_obs.detach().cpu()\n",
    "    miss_idx = (1 - mask_cpu[idx_stat]).bool()  # (1,28,28) bool\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        if method == \"pseudo\":\n",
    "            x_out, _ = pseudo_gibbs_step(model, x, mask_obs)\n",
    "            acc = np.nan\n",
    "        else:\n",
    "            x_out, z, acc = metropolis_within_gibbs_step(\n",
    "                model, x, z, mask_obs,\n",
    "                return_accept=True,\n",
    "                adaptive=False,\n",
    "                proposal_scale=proposal_scale\n",
    "            )\n",
    "\n",
    "        x = torch.bernoulli(x_out)\n",
    "\n",
    "        keep = (t >= burn_in) and ((t - burn_in) % thinning == 0)\n",
    "        if keep:\n",
    "            # logp at this kept sample (mean over batch)\n",
    "            ll = bernoulli_ll_missing(x_true, x_out, mask_obs)   # (B,) CPU\n",
    "            logp = float(ll.mean().item())\n",
    "\n",
    "            # K = number of ones in missing region for idx_stat (use binary x)\n",
    "            x_cpu = x.detach().cpu()\n",
    "            k = float(x_cpu[idx_stat][miss_idx].sum().item())\n",
    "\n",
    "            steps_kept.append(t)\n",
    "            logp_list.append(logp)\n",
    "            k_list.append(k)\n",
    "            acc_list.append(float(acc) if method == \"mwg\" else np.nan)\n",
    "\n",
    "    return {\n",
    "        \"steps_kept\": np.array(steps_kept),\n",
    "        \"logp\": np.array(logp_list),\n",
    "        \"k\": np.array(k_list),\n",
    "        \"acc\": np.array(acc_list),\n",
    "    }\n",
    "def plot_trace_and_acf(trace, outdir, tag, max_lag=200):\n",
    "    steps = trace[\"steps_kept\"]\n",
    "    logp = trace[\"logp\"]\n",
    "    k = trace[\"k\"]\n",
    "    acc = trace[\"acc\"]\n",
    "\n",
    "    # Trace logp\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.plot(steps, logp, \"-o\", markersize=3)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"mean log p(x_miss_true | sample)\")\n",
    "    plt.title(f\"{tag} — trace logp\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    savefig(outdir, f\"{tag}_trace_logp.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Trace k\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.plot(steps, k, \"-o\", markersize=3)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"#ones in missing region (idx)\")\n",
    "    plt.title(f\"{tag} — trace K (#ones missing)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    savefig(outdir, f\"{tag}_trace_k.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ACF logp\n",
    "    lags, ac = acf_1d(logp, max_lag=max_lag)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(lags, ac, \"-o\", markersize=3)\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.ylabel(\"ACF\")\n",
    "    plt.title(f\"{tag} — ACF(logp)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    savefig(outdir, f\"{tag}_acf_logp.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ACF k\n",
    "    lags, ac = acf_1d(k, max_lag=max_lag)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(lags, ac, \"-o\", markersize=3)\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.ylabel(\"ACF\")\n",
    "    plt.title(f\"{tag} — ACF(K)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    savefig(outdir, f\"{tag}_acf_k.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # acceptance trace (MwG)\n",
    "    if np.isfinite(acc).any():\n",
    "        plt.figure(figsize=(9,3))\n",
    "        plt.plot(steps, acc, \"-o\", markersize=3)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"accept\")\n",
    "        plt.title(f\"{tag} — MwG accept (at kept steps)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        savefig(outdir, f\"{tag}_accept.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # ESS\n",
    "    ess_logp = ess_from_acf(logp, max_lag=max_lag)\n",
    "    ess_k = ess_from_acf(k, max_lag=max_lag)\n",
    "\n",
    "    return {\"ess_logp\": ess_logp, \"ess_k\": ess_k, \"n_kept\": len(logp)}\n",
    "outdir = exp5_outdir()\n",
    "\n",
    "# ---- Choix du scénario ----\n",
    "mask_kind = \"top\"        # \"top\" ou \"random50\"\n",
    "missing_rate = 0.5       # utilisé si random50\n",
    "\n",
    "n_iters = 12000\n",
    "burn_in = 2000\n",
    "thinning = 20\n",
    "warmup_pg = 50\n",
    "proposal_scale = 1.0\n",
    "idx_stat = 0             # quel exemple du batch pour K\n",
    "\n",
    "# mask + init\n",
    "if mask_kind == \"random50\":\n",
    "    mask = make_random_mask(x_true, missing_rate=missing_rate)\n",
    "else:\n",
    "    mask = make_structured_mask(x_true, kind=\"top\")\n",
    "x_init = init_with_noise(x_true, mask)\n",
    "\n",
    "# ---- 1) Pseudo trace ----\n",
    "trace_p = collect_trace(\n",
    "    model, x_true, x_init, mask,\n",
    "    method=\"pseudo\",\n",
    "    n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "    idx_stat=idx_stat\n",
    ")\n",
    "ess_p = plot_trace_and_acf(trace_p, outdir, tag=f\"pseudo_{mask_kind}\", max_lag=200)\n",
    "\n",
    "# ---- 2) MwG trace ----\n",
    "trace_m = collect_trace(\n",
    "    model, x_true, x_init, mask,\n",
    "    method=\"mwg\",\n",
    "    n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "    warmup_pg=warmup_pg,\n",
    "    proposal_scale=proposal_scale,\n",
    "    idx_stat=idx_stat\n",
    ")\n",
    "ess_m = plot_trace_and_acf(trace_m, outdir, tag=f\"mwg_{mask_kind}\", max_lag=200)\n",
    "\n",
    "# Save raw traces\n",
    "pd.DataFrame({\n",
    "    \"step\": trace_p[\"steps_kept\"],\n",
    "    \"pseudo_logp\": trace_p[\"logp\"],\n",
    "    \"pseudo_k\": trace_p[\"k\"],\n",
    "}).to_csv(os.path.join(outdir, f\"trace_pseudo_{mask_kind}.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"step\": trace_m[\"steps_kept\"],\n",
    "    \"mwg_logp\": trace_m[\"logp\"],\n",
    "    \"mwg_k\": trace_m[\"k\"],\n",
    "    \"mwg_acc\": trace_m[\"acc\"],\n",
    "}).to_csv(os.path.join(outdir, f\"trace_mwg_{mask_kind}.csv\"), index=False)\n",
    "\n",
    "# Summary table ESS\n",
    "df_ess = pd.DataFrame([\n",
    "    {\"method\": \"pseudo\", \"mask\": mask_kind, **ess_p},\n",
    "    {\"method\": \"mwg\", \"mask\": mask_kind, **ess_m},\n",
    "])\n",
    "df_ess.to_csv(os.path.join(outdir, f\"ess_summary_{mask_kind}.csv\"), index=False)\n",
    "print(df_ess)\n",
    "\n",
    "# ---- 3) Multi-chain R-hat (MwG) ----\n",
    "n_chains = 5\n",
    "chains_logp = []\n",
    "chains_k = []\n",
    "\n",
    "for c in range(n_chains):\n",
    "    tr = collect_trace(\n",
    "        model, x_true, x_init, mask,\n",
    "        method=\"mwg\",\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        warmup_pg=warmup_pg,\n",
    "        proposal_scale=proposal_scale,\n",
    "        idx_stat=idx_stat\n",
    "    )\n",
    "    chains_logp.append(tr[\"logp\"])\n",
    "    chains_k.append(tr[\"k\"])\n",
    "\n",
    "# Rhat on logp and k\n",
    "Rhat_logp = rhat(chains_logp)\n",
    "Rhat_k = rhat(chains_k)\n",
    "\n",
    "df_rhat = pd.DataFrame([{\n",
    "    \"mask\": mask_kind,\n",
    "    \"n_chains\": n_chains,\n",
    "    \"Rhat_logp\": Rhat_logp,\n",
    "    \"Rhat_k\": Rhat_k\n",
    "}])\n",
    "df_rhat.to_csv(os.path.join(outdir, f\"rhat_{mask_kind}.csv\"), index=False)\n",
    "print(df_rhat)\n",
    "\n",
    "# Plot overlay of chains (logp)\n",
    "plt.figure(figsize=(9,3))\n",
    "for i, c in enumerate(chains_logp):\n",
    "    plt.plot(c, label=f\"chain{i}\", alpha=0.7)\n",
    "plt.title(f\"MwG chains logp overlay (mask={mask_kind}) | Rhat={Rhat_logp:.3f}\")\n",
    "plt.xlabel(\"kept sample index\")\n",
    "plt.ylabel(\"logp\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(ncol=3, fontsize=8)\n",
    "savefig(outdir, f\"mwg_multichain_logp_overlay_{mask_kind}.png\")\n",
    "plt.show()\n",
    "# ============================================\n",
    "# EXP5-BIS : ESS / R-hat vs difficulté (p)\n",
    "# ============================================\n",
    "# Objectif: pour p in {0.1,0.3,0.5,0.7,0.9}, on calcule\n",
    "# - ESS(logp) et ESS(K) pour Pseudo et MwG\n",
    "# - Rhat(logp) et Rhat(K) sur MwG multi-chaînes\n",
    "# + sauvegarde CSV + figures dans results/exp5_sweep_p_<timestamp>/\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Output dir ----------\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outdir = os.path.join(\"results\", f\"exp5_sweep_p_{run_id}\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "print(\"OUT:\", outdir)\n",
    "\n",
    "def savefig(name, dpi=200):\n",
    "    path = os.path.join(outdir, name)\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"saved:\", path)\n",
    "\n",
    "# --------- Settings ----------\n",
    "p_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Chain settings (diagnostics: mieux d'avoir assez de points gardés)\n",
    "n_iters   = 20000\n",
    "burn_in   = 5000\n",
    "thinning  = 50\n",
    "warmup_pg = 50\n",
    "proposal_scale = 1.0\n",
    "idx_stat = 0\n",
    "\n",
    "# Multi-chain for Rhat\n",
    "n_chains = 5\n",
    "\n",
    "# ACF / ESS settings\n",
    "max_lag = 200\n",
    "\n",
    "# --------- Run sweep ----------\n",
    "rows = []\n",
    "overlay_dir = os.path.join(outdir, \"overlays\")\n",
    "os.makedirs(overlay_dir, exist_ok=True)\n",
    "\n",
    "for p in p_list:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"p_missing = {p}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # mask + init\n",
    "    mask = make_random_mask(x_true, missing_rate=p)\n",
    "    x_init = init_with_noise(x_true, mask)\n",
    "\n",
    "    # ---- Pseudo trace ----\n",
    "    tr_p = collect_trace(\n",
    "        model, x_true, x_init, mask,\n",
    "        method=\"pseudo\",\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        idx_stat=idx_stat\n",
    "    )\n",
    "\n",
    "    ess_p_logp = ess_from_acf(tr_p[\"logp\"], max_lag=max_lag)\n",
    "    ess_p_k    = ess_from_acf(tr_p[\"k\"],    max_lag=max_lag)\n",
    "\n",
    "    # ---- MwG trace (single chain for ESS) ----\n",
    "    tr_m = collect_trace(\n",
    "        model, x_true, x_init, mask,\n",
    "        method=\"mwg\",\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        warmup_pg=warmup_pg,\n",
    "        proposal_scale=proposal_scale,\n",
    "        idx_stat=idx_stat\n",
    "    )\n",
    "\n",
    "    ess_m_logp = ess_from_acf(tr_m[\"logp\"], max_lag=max_lag)\n",
    "    ess_m_k    = ess_from_acf(tr_m[\"k\"],    max_lag=max_lag)\n",
    "\n",
    "    acc_mean = float(np.nanmean(tr_m[\"acc\"])) if np.isfinite(tr_m[\"acc\"]).any() else np.nan\n",
    "\n",
    "    # ---- MwG multi-chain for R-hat ----\n",
    "    chains_logp, chains_k = [], []\n",
    "    for c in range(n_chains):\n",
    "        tr_c = collect_trace(\n",
    "            model, x_true, x_init, mask,\n",
    "            method=\"mwg\",\n",
    "            n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "            warmup_pg=warmup_pg,\n",
    "            proposal_scale=proposal_scale,\n",
    "            idx_stat=idx_stat\n",
    "        )\n",
    "        chains_logp.append(tr_c[\"logp\"])\n",
    "        chains_k.append(tr_c[\"k\"])\n",
    "\n",
    "    Rhat_logp = rhat(chains_logp)\n",
    "    Rhat_k    = rhat(chains_k)\n",
    "\n",
    "    # ---- Save overlay figure (logp) ----\n",
    "    plt.figure(figsize=(9,3))\n",
    "    for i, c in enumerate(chains_logp):\n",
    "        plt.plot(c, alpha=0.7, label=f\"c{i}\")\n",
    "    plt.title(f\"MwG chains logp overlay | p={p} | Rhat={Rhat_logp:.3f}\")\n",
    "    plt.xlabel(\"kept sample index\"); plt.ylabel(\"logp\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(ncol=5, fontsize=8)\n",
    "    overlay_path = os.path.join(\"overlays\", f\"mwg_overlay_logp_p{int(p*100)}.png\")\n",
    "    plt.savefig(os.path.join(outdir, overlay_path), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"saved:\", os.path.join(outdir, overlay_path))\n",
    "\n",
    "    # ---- Save traces (optional but useful) ----\n",
    "    pd.DataFrame({\n",
    "        \"step\": tr_p[\"steps_kept\"],\n",
    "        \"pseudo_logp\": tr_p[\"logp\"],\n",
    "        \"pseudo_k\": tr_p[\"k\"],\n",
    "    }).to_csv(os.path.join(outdir, f\"trace_pseudo_p{int(p*100)}.csv\"), index=False)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"step\": tr_m[\"steps_kept\"],\n",
    "        \"mwg_logp\": tr_m[\"logp\"],\n",
    "        \"mwg_k\": tr_m[\"k\"],\n",
    "        \"mwg_acc\": tr_m[\"acc\"],\n",
    "    }).to_csv(os.path.join(outdir, f\"trace_mwg_p{int(p*100)}.csv\"), index=False)\n",
    "\n",
    "    # ---- Summary row ----\n",
    "    rows.append({\n",
    "        \"p_missing\": p,\n",
    "        \"n_kept\": len(tr_m[\"logp\"]),\n",
    "        \"acc_mwg_mean\": acc_mean,\n",
    "        \"ess_pseudo_logp\": ess_p_logp,\n",
    "        \"ess_pseudo_k\": ess_p_k,\n",
    "        \"ess_mwg_logp\": ess_m_logp,\n",
    "        \"ess_mwg_k\": ess_m_k,\n",
    "        \"rhat_mwg_logp\": Rhat_logp,\n",
    "        \"rhat_mwg_k\": Rhat_k\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(outdir, \"exp5_sweep_p_summary.csv\"), index=False)\n",
    "print(\"\\nSaved summary:\", os.path.join(outdir, \"exp5_sweep_p_summary.csv\"))\n",
    "print(df)\n",
    "\n",
    "# --------- Plot ESS vs p ----------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"p_missing\"], df[\"ess_pseudo_logp\"], \"--o\", label=\"Pseudo ESS(logp)\")\n",
    "plt.plot(df[\"p_missing\"], df[\"ess_mwg_logp\"], \"-o\", label=\"MwG ESS(logp)\")\n",
    "plt.xlabel(\"p_missing\")\n",
    "plt.ylabel(\"ESS (logp)\")\n",
    "plt.title(\"ESS(logp) vs missing rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "savefig(\"ess_logp_vs_p.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"p_missing\"], df[\"ess_pseudo_k\"], \"--o\", label=\"Pseudo ESS(K)\")\n",
    "plt.plot(df[\"p_missing\"], df[\"ess_mwg_k\"], \"-o\", label=\"MwG ESS(K)\")\n",
    "plt.xlabel(\"p_missing\")\n",
    "plt.ylabel(\"ESS (K)\")\n",
    "plt.title(\"ESS(K) vs missing rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "savefig(\"ess_k_vs_p.png\")\n",
    "plt.show()\n",
    "\n",
    "# --------- Plot Rhat vs p ----------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"p_missing\"], df[\"rhat_mwg_logp\"], \"-o\")\n",
    "plt.axhline(1.0, linestyle=\"--\")\n",
    "plt.xlabel(\"p_missing\")\n",
    "plt.ylabel(\"R-hat (logp)\")\n",
    "plt.title(\"MwG R-hat(logp) vs missing rate (closer to 1 is better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(\"rhat_logp_vs_p.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"p_missing\"], df[\"rhat_mwg_k\"], \"-o\")\n",
    "plt.axhline(1.0, linestyle=\"--\")\n",
    "plt.xlabel(\"p_missing\")\n",
    "plt.ylabel(\"R-hat (K)\")\n",
    "plt.title(\"MwG R-hat(K) vs missing rate (closer to 1 is better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(\"rhat_k_vs_p.png\")\n",
    "plt.show()\n",
    "\n",
    "# --------- Plot acceptance vs p ----------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"p_missing\"], df[\"acc_mwg_mean\"], \"-o\")\n",
    "plt.xlabel(\"p_missing\")\n",
    "plt.ylabel(\"MwG acceptance (mean over kept steps)\")\n",
    "plt.title(\"MwG acceptance vs missing rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(\"accept_vs_p.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDONE. Everything saved to:\", outdir)\n",
    "import os, datetime, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sampler import pseudo_gibbs_step, metropolis_within_gibbs_step\n",
    "from sampler import metropolis_within_gibbs_step_mixture  # nouveau !\n",
    "\n",
    "def make_random_mask(x, missing_rate: float):\n",
    "    keep_prob = 1.0 - missing_rate\n",
    "    return torch.bernoulli(torch.full_like(x, keep_prob))\n",
    "\n",
    "def make_structured_mask(x, kind=\"top\"):\n",
    "    m = torch.ones_like(x)\n",
    "    if kind == \"top\":\n",
    "        m[:, :, :14, :] = 0\n",
    "    elif kind == \"bottom\":\n",
    "        m[:, :, 14:, :] = 0\n",
    "    elif kind == \"center\":\n",
    "        m[:, :, 8:20, 8:20] = 0\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return m\n",
    "\n",
    "def init_with_noise(x_true, mask_obs):\n",
    "    noise = torch.bernoulli(torch.full_like(x_true, 0.5))\n",
    "    return x_true * mask_obs + noise * (1 - mask_obs)\n",
    "\n",
    "def savefig(outdir, name, dpi=200):\n",
    "    path = os.path.join(outdir, name)\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"saved:\", path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_mwg_variant_mean_last(\n",
    "    model, x_true, x_init, mask_obs,\n",
    "    *,\n",
    "    variant=\"base\",        # \"base\" or \"mixture\"\n",
    "    n_iters=12000,\n",
    "    burn_in=2000,\n",
    "    thinning=20,\n",
    "    warmup_pg=50,\n",
    "    proposal_scale=1.0,    # base mwg\n",
    "    alpha=0.5,             # mixture\n",
    "    rw_sigma=0.5,\n",
    "):\n",
    "    t0 = time.perf_counter()\n",
    "    x = x_init.clone()\n",
    "\n",
    "    # init z\n",
    "    for _ in range(warmup_pg):\n",
    "        x_out, _ = pseudo_gibbs_step(model, x, mask_obs)\n",
    "        x = torch.bernoulli(x_out)\n",
    "    mu, logvar = model.encode(x.view(-1, 784))\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "\n",
    "    sum_mean = None\n",
    "    n_kept = 0\n",
    "    logS = None\n",
    "    last_bin = None\n",
    "    acc_hist = []\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        if variant == \"base\":\n",
    "            x_out, z, acc = metropolis_within_gibbs_step(\n",
    "                model, x, z, mask_obs,\n",
    "                return_accept=True,\n",
    "                adaptive=False,\n",
    "                proposal_scale=proposal_scale\n",
    "            )\n",
    "        elif variant == \"mixture\":\n",
    "            x_out, z, acc = metropolis_within_gibbs_step_mixture(\n",
    "                model, x, z, mask_obs,\n",
    "                alpha=alpha,\n",
    "                rw_sigma=rw_sigma,\n",
    "                return_accept=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"variant must be base or mixture\")\n",
    "\n",
    "        x = torch.bernoulli(x_out)\n",
    "        last_bin = x.detach().cpu()\n",
    "        acc_hist.append(acc)\n",
    "\n",
    "        keep = (t >= burn_in) and ((t - burn_in) % thinning == 0)\n",
    "        if keep:\n",
    "            if sum_mean is None:\n",
    "                sum_mean = x_out.detach().clone()\n",
    "            else:\n",
    "                sum_mean += x_out.detach()\n",
    "            n_kept += 1\n",
    "\n",
    "            ll = bernoulli_ll_missing(x_true, x_out, mask_obs)  # CPU robust\n",
    "            if logS is None:\n",
    "                logS = ll.clone()\n",
    "            else:\n",
    "                logS = torch.logaddexp(logS, ll)\n",
    "\n",
    "    mean_probs = (sum_mean / max(n_kept,1)).detach().cpu()\n",
    "    f1_mean = f1_missing(x_true, mean_probs.to(device), mask_obs)\n",
    "    f1_last = f1_missing(x_true, last_bin.to(device), mask_obs)\n",
    "    logp_mc = float((logS - math.log(max(n_kept, 1))).mean().item()) if logS is not None else float(\"nan\")\n",
    "    acc_mean = float(np.mean(acc_hist[-max(1, len(acc_hist)//10):]))  # dernier 10%\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    return {\n",
    "        \"f1_mean\": f1_mean,\n",
    "        \"f1_last\": f1_last,\n",
    "        \"logp_mc\": logp_mc,\n",
    "        \"acc_mean\": acc_mean,\n",
    "        \"time\": dt,\n",
    "        \"kept\": n_kept,\n",
    "        \"mean_probs\": mean_probs,\n",
    "        \"last_bin\": last_bin\n",
    "    }\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outdir = os.path.join(\"results\", f\"exp6_mixture_alpha_{run_id}\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "print(\"OUT:\", outdir)\n",
    "\n",
    "# --- Choix du masque ---\n",
    "mask_kind = \"top\"     # \"top\" ou \"random50\"\n",
    "missing_rate = 0.5    # utilisé si random50\n",
    "\n",
    "if mask_kind == \"random50\":\n",
    "    mask = make_random_mask(x_true, missing_rate=missing_rate)\n",
    "else:\n",
    "    mask = make_structured_mask(x_true, kind=\"top\")\n",
    "\n",
    "x_init = init_with_noise(x_true, mask)\n",
    "\n",
    "# --- Params ---\n",
    "n_iters=12000\n",
    "burn_in=2000\n",
    "thinning=20\n",
    "warmup_pg=50\n",
    "\n",
    "# baseline MwG (base)\n",
    "base = run_mwg_variant_mean_last(\n",
    "    model, x_true, x_init, mask,\n",
    "    variant=\"base\",\n",
    "    n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "    warmup_pg=warmup_pg,\n",
    "    proposal_scale=1.0\n",
    ")\n",
    "\n",
    "alpha_list = [0.0, 0.25, 0.5, 0.75, 1.0]  # 0=pur RW, 1=pur independence\n",
    "rw_sigma = 0.5\n",
    "\n",
    "rows = [{\n",
    "    \"variant\": \"base\",\n",
    "    \"alpha\": np.nan,\n",
    "    \"rw_sigma\": np.nan,\n",
    "    \"f1_mean\": base[\"f1_mean\"],\n",
    "    \"f1_last\": base[\"f1_last\"],\n",
    "    \"logp_mc\": base[\"logp_mc\"],\n",
    "    \"acc\": base[\"acc_mean\"],\n",
    "    \"time_s\": base[\"time\"],\n",
    "    \"kept\": base[\"kept\"],\n",
    "}]\n",
    "\n",
    "for a in alpha_list:\n",
    "    res = run_mwg_variant_mean_last(\n",
    "        model, x_true, x_init, mask,\n",
    "        variant=\"mixture\",\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        warmup_pg=warmup_pg,\n",
    "        alpha=a,\n",
    "        rw_sigma=rw_sigma\n",
    "    )\n",
    "    rows.append({\n",
    "        \"variant\": \"mixture\",\n",
    "        \"alpha\": a,\n",
    "        \"rw_sigma\": rw_sigma,\n",
    "        \"f1_mean\": res[\"f1_mean\"],\n",
    "        \"f1_last\": res[\"f1_last\"],\n",
    "        \"logp_mc\": res[\"logp_mc\"],\n",
    "        \"acc\": res[\"acc_mean\"],\n",
    "        \"time_s\": res[\"time\"],\n",
    "        \"kept\": res[\"kept\"],\n",
    "    })\n",
    "    print(f\"alpha={a:.2f} | F1={res['f1_mean']:.3f} logp={res['logp_mc']:.1f} acc={res['acc_mean']:.3f}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(outdir, \"exp6_alpha_sweep.csv\"), index=False)\n",
    "print(df)\n",
    "\n",
    "# Plots\n",
    "df_mix = df[df[\"variant\"]==\"mixture\"].copy()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_mix[\"alpha\"], df_mix[\"f1_mean\"], \"-o\")\n",
    "plt.axhline(base[\"f1_mean\"], linestyle=\"--\", label=\"base MwG\")\n",
    "plt.xlabel(\"alpha (independence weight)\")\n",
    "plt.ylabel(\"F1 (mean)\")\n",
    "plt.title(f\"Exp6: F1 vs alpha (mask={mask_kind}, rw_sigma={rw_sigma})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "savefig(outdir, \"f1_vs_alpha.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_mix[\"alpha\"], df_mix[\"logp_mc\"], \"-o\")\n",
    "plt.axhline(base[\"logp_mc\"], linestyle=\"--\", label=\"base MwG\")\n",
    "plt.xlabel(\"alpha (independence weight)\")\n",
    "plt.ylabel(\"MC log-likelihood\")\n",
    "plt.title(f\"Exp6: logp vs alpha (mask={mask_kind}, rw_sigma={rw_sigma})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "savefig(outdir, \"logp_vs_alpha.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_mix[\"alpha\"], df_mix[\"acc\"], \"-o\")\n",
    "plt.axhline(base[\"acc_mean\"], linestyle=\"--\", label=\"base MwG\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"acceptance (last-window)\")\n",
    "plt.title(f\"Exp6: acceptance vs alpha (mask={mask_kind}, rw_sigma={rw_sigma})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "savefig(outdir, \"acc_vs_alpha.png\")\n",
    "plt.show()\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outdir2 = os.path.join(\"results\", f\"exp6_mixture_sigma_{run_id}\")\n",
    "os.makedirs(outdir2, exist_ok=True)\n",
    "print(\"OUT:\", outdir2)\n",
    "\n",
    "alpha = 0.5\n",
    "sigma_list = [0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "rows = []\n",
    "for s in sigma_list:\n",
    "    res = run_mwg_variant_mean_last(\n",
    "        model, x_true, x_init, mask,\n",
    "        variant=\"mixture\",\n",
    "        n_iters=n_iters, burn_in=burn_in, thinning=thinning,\n",
    "        warmup_pg=warmup_pg,\n",
    "        alpha=alpha,\n",
    "        rw_sigma=s\n",
    "    )\n",
    "    rows.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"rw_sigma\": s,\n",
    "        \"f1_mean\": res[\"f1_mean\"],\n",
    "        \"logp_mc\": res[\"logp_mc\"],\n",
    "        \"acc\": res[\"acc_mean\"],\n",
    "        \"time_s\": res[\"time\"],\n",
    "        \"kept\": res[\"kept\"]\n",
    "    })\n",
    "    print(f\"sigma={s:.2f} | F1={res['f1_mean']:.3f} logp={res['logp_mc']:.1f} acc={res['acc_mean']:.3f}\")\n",
    "\n",
    "df2 = pd.DataFrame(rows)\n",
    "df2.to_csv(os.path.join(outdir2, \"exp6_sigma_sweep.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df2[\"rw_sigma\"], df2[\"f1_mean\"], \"-o\")\n",
    "plt.xlabel(\"rw_sigma\"); plt.ylabel(\"F1 (mean)\")\n",
    "plt.title(f\"Exp6: F1 vs rw_sigma (alpha={alpha}, mask={mask_kind})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(outdir2, \"f1_vs_sigma.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df2[\"rw_sigma\"], df2[\"logp_mc\"], \"-o\")\n",
    "plt.xlabel(\"rw_sigma\"); plt.ylabel(\"MC log-likelihood\")\n",
    "plt.title(f\"Exp6: logp vs rw_sigma (alpha={alpha}, mask={mask_kind})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(outdir2, \"logp_vs_sigma.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df2[\"rw_sigma\"], df2[\"acc\"], \"-o\")\n",
    "plt.xlabel(\"rw_sigma\"); plt.ylabel(\"acceptance\")\n",
    "plt.title(f\"Exp6: acceptance vs rw_sigma (alpha={alpha}, mask={mask_kind})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "savefig(outdir2, \"acc_vs_sigma.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
